{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1cf32c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae6df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import jieba  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be428c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean single text: Retain Chinese characters and common punctuation marks, and remove HTML tags, advertisements, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Retain Chinese characters, common punctuation, numbers, and letters (optional)\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€˜â€™â€œâ€ï¼ˆï¼‰ã€ã€‘\\n\\s0-9a-zA-Z]', '', text)\n",
    "    # Remove extra spaces and newlines\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def preprocess_corpus(df):\n",
    "    \"\"\"\n",
    "    Preprocess the entire DataFrame and return a list containing all cleaned texts.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title_clean = clean_text(row['æ–‡æœ¬æ ‡é¢˜'])\n",
    "        content_clean = clean_text(row['æ–‡æœ¬å†…å®¹'])\n",
    "        # Merge the title and content and separate them with spaces\n",
    "        full_text = f\"{title_clean} {content_clean}\"\n",
    "        corpus.append(full_text)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52626b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cooccurrence_prob(corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    Calculate the probability of any two Chinese characters co-occurring adjacently P(c_j | c_i)\n",
    "    Returns a dictionary: {(c_i, c_j): probability}\n",
    "    \"\"\"\n",
    "    char_count = defaultdict(int)      # Single character frequency\n",
    "    cooccur_count = defaultdict(lambda: defaultdict(int))  # Co-occurrence frequency\n",
    "    for text in corpus:\n",
    "        # Iterate over each character\n",
    "        for i in range(len(text) - window_size + 1):\n",
    "            window = text[i:i+window_size]\n",
    "            if len(window) == window_size:\n",
    "                c1, c2 = window[0], window[1]\n",
    "                char_count[c1] += 1\n",
    "                cooccur_count[c1][c2] += 1\n",
    "\n",
    "    # Calculate the conditional probability P(c_j | c_i)\n",
    "    prob_dict = {}\n",
    "    for c1 in cooccur_count:\n",
    "        for c2 in cooccur_count[c1]:\n",
    "            prob_dict[(c1, c2)] = cooccur_count[c1][c2] / char_count[c1]\n",
    "\n",
    "    return prob_dict, char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2c6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pmi(cooccur_prob_dict, char_count, total_chars):\n",
    "    \"\"\"\n",
    "    Calculate pointwise mutual information PMI(c_i, c_j) = log2( P(c_i, c_j) / (P(c_i) * P(c_j)) )\n",
    "    Note: Here P(c_i, c_j) = P(c_j|c_i) * P(c_i)\n",
    "    \"\"\"\n",
    "    pmi_dict = {}\n",
    "    for (c1, c2), cooccur_prob in cooccur_prob_dict.items():\n",
    "        p_c1 = char_count[c1] / total_chars\n",
    "        p_c2 = char_count[c2] / total_chars\n",
    "        # P(c1,c2) = P(c2|c1) * P(c1)\n",
    "        p_joint = cooccur_prob * p_c1\n",
    "        if p_c1 > 0 and p_c2 > 0 and p_joint > 0:\n",
    "            pmi = np.log2(p_joint / (p_c1 * p_c2))\n",
    "            pmi_dict[(c1, c2)] = pmi\n",
    "    return pmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_vector(article_text, vocab):\n",
    "    \"\"\"\n",
    "    Convert the article into a word frequency vector (based on the given vocabulary list)\n",
    "    \"\"\"\n",
    "    # Word segmentation (optional; if characters are used, traverse directly)\n",
    "    # words = list(article_text)  # Character-level\n",
    "    # use jieba for word segmentation\n",
    "    words = jieba.lcut(article_text)\n",
    "    \n",
    "    word_count = Counter(words)\n",
    "    vector = np.array([word_count.get(word, 0) for word in vocab])\n",
    "    # Normalize to probability distribution\n",
    "    if np.sum(vector) > 0:\n",
    "        vector = vector / np.sum(vector)\n",
    "    return vector\n",
    "\n",
    "def avg_mutual_information_precise(article1, article2, vocab):\n",
    "    \"\"\"\n",
    "    To calculate the average mutual information between two articles more precisely (a joint distribution needs to be constructed)\n",
    "    This method calculates the \"average mutual information at the lexical level\".\n",
    "    \"\"\"\n",
    "    vec1 = article_to_vector(article1, vocab)\n",
    "    vec2 = article_to_vector(article2, vocab)\n",
    "    \n",
    "    # Construct joint distribution (simplified: assume words are independent, but they are not)\n",
    "    # The true joint distribution requires counting the co-occurrence of each pair of words in the two articles.\n",
    "    # Since we only have two articles, we cannot construct a reliable joint distribution.\n",
    "    # Therefore, we switch to calculating the \"average pointwise mutual information of word co-occurrence\".\n",
    "    \n",
    "    # Extract all words from the two articles\n",
    "    words1 = set(jieba.lcut(article1) if 'jieba' in globals() else list(article1))\n",
    "    words2 = set(jieba.lcut(article2) if 'jieba' in globals() else list(article2))\n",
    "    \n",
    "    # Calculate the PMI of the intersecting words (requires global corpus statistics)\n",
    "    # Since we do not have a global corpus, here we can only provide a framework.\n",
    "    # In actual applications, the global char_count and cooccur_count calculated earlier should be used.\n",
    "    \n",
    "    # We output a simple metric based on word overlap\n",
    "    overlap = len(words1 & words2) / len(words1 | words2)  # Jaccard similarity\n",
    "    return overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30441e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main flow function: From the original DataFrame to all calculation results\n",
    "    \"\"\"\n",
    "    print(\"Start data cleaning...\")\n",
    "    corpus = preprocess_corpus(df)\n",
    "    print(f\"Cleaning completed, total {len(corpus)} articles\")\n",
    "\n",
    "    print(\"\\nStart calculating adjacent co-occurrence probability...\")\n",
    "    cooccur_prob, char_count = calculate_cooccurrence_prob(corpus)\n",
    "    total_chars = sum(char_count.values())\n",
    "    print(f\"Total {len(char_count)} different Chinese characters counted\")\n",
    "\n",
    "    print(\"\\nStart calculating pointwise mutual information...\")\n",
    "    pmi_result = calculate_pmi(cooccur_prob, char_count, total_chars)\n",
    "    print(f\"Total {len(pmi_result)} Chinese character pairs' PMI calculated\")\n",
    "\n",
    "    print(\"\\nStart calculating average mutual information between articles...\")\n",
    "    # Calculate the average mutual information between the first two articles as an example\n",
    "    if len(corpus) >= 2:\n",
    "        article1 = corpus[0]\n",
    "        article2 = corpus[1]\n",
    "        vocab = list(set(list(article1) + list(article2)))\n",
    "        avg_mi = avg_mutual_information_precise(article1, article2, vocab)\n",
    "        print(f\"Average mutual information between article 1 and article 2 (Jaccard approximation): {avg_mi:.6f}\")\n",
    "    else:\n",
    "        print(\"Insufficient number of articles to calculate mutual information between articles\")\n",
    "\n",
    "    # Return all results for further analysis or saving\n",
    "    return {\n",
    "        'corpus': corpus,\n",
    "        'cooccur_prob': cooccur_prob,\n",
    "        'char_count': char_count,\n",
    "        'pmi_result': pmi_result,\n",
    "        'total_chars': total_chars\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed9a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data cleaning...\n",
      "Cleaning completed, total 975 articles\n",
      "\n",
      "Start calculating adjacent co-occurrence probability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/v2/ptjfpzrd3pz9s7p0353vdcvw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3526 different Chinese characters counted\n",
      "\n",
      "Start calculating pointwise mutual information...\n",
      "Total 181393 Chinese character pairs' PMI calculated\n",
      "\n",
      "Start calculating average mutual information between articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.237 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mutual information between article 1 and article 2 (Jaccard approximation): 0.110977\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df = pd.read_csv('./data/xinlangnews.csv')\n",
    "    res = main_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a205c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved corpus to: ./data/xinlangnews_analysis_results_corpus.txt\n",
      "âœ“ Saved cooccur_prob to: ./data/xinlangnews_analysis_results_cooccur_prob.json\n",
      "âœ“ Saved char_count to: ./data/xinlangnews_analysis_results_char_count.json\n",
      "âœ“ Saved pmi_result to: ./data/xinlangnews_analysis_results_pmi_result.json\n",
      "âœ“ Saved pmi_result to: ./data/xinlangnews_analysis_results_pmi_result.csv\n",
      "âœ“ Saved metadata to: ./data/xinlangnews_analysis_results_metadata.json\n",
      "\n",
      "ğŸ‰ Results have been saved to ./data/xinlangnews_analysis_results_* files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy data types to native Python types\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {str(k): convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # For other numpy types\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_results_separate(results, base_filename):\n",
    "    \"\"\"\n",
    "    Save different components in appropriate formats (fixed version)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Save corpus (text data)\n",
    "    if 'corpus' in results and results['corpus'] is not None:\n",
    "        try:\n",
    "            with open(f'./data/{base_filename}_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "                if isinstance(results['corpus'], list):\n",
    "                    for item in results['corpus']:\n",
    "                        f.write(str(item) + '\\n')\n",
    "                else:\n",
    "                    f.write(str(results['corpus']))\n",
    "            print(f\"âœ“ Saved corpus to: ./data/{base_filename}_corpus.txt\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save corpus: {e}\")\n",
    "    \n",
    "    # 2. Save cooccur_prob\n",
    "    if 'cooccur_prob' in results and results['cooccur_prob'] is not None:\n",
    "        try:\n",
    "            if hasattr(results['cooccur_prob'], 'to_csv'):\n",
    "                results['cooccur_prob'].to_csv(f'./data/{base_filename}_cooccur_prob.csv', encoding='utf-8')\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.csv\")\n",
    "            elif isinstance(results['cooccur_prob'], (pd.DataFrame, np.ndarray)):\n",
    "                pd.DataFrame(results['cooccur_prob']).to_csv(f'./data/{base_filename}_cooccur_prob.csv')\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.csv\")\n",
    "            else:\n",
    "                # If it's a dictionary or other type\n",
    "                converted_data = convert_numpy_types(results['cooccur_prob'])\n",
    "                with open(f'./data/{base_filename}_cooccur_prob.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save cooccur_prob: {e}\")\n",
    "    \n",
    "    # 3. Save char_count (dictionary data)\n",
    "    if 'char_count' in results and results['char_count'] is not None:\n",
    "        try:\n",
    "            converted_char_count = convert_numpy_types(results['char_count'])\n",
    "            with open(f'./data/{base_filename}_char_count.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(converted_char_count, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved char_count to: ./data/{base_filename}_char_count.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save char_count: {e}\")\n",
    "    \n",
    "    # 4. Save pmi_result (fixed version)\n",
    "    if 'pmi_result' in results and results['pmi_result'] is not None:\n",
    "        try:\n",
    "            # First convert numpy types\n",
    "            converted_pmi = convert_numpy_types(results['pmi_result'])\n",
    "            \n",
    "            # Save as JSON\n",
    "            with open(f'./data/{base_filename}_pmi_result.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(converted_pmi, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved pmi_result to: ./data/{base_filename}_pmi_result.json\")\n",
    "            \n",
    "            # Also save as CSV for easier viewing\n",
    "            if isinstance(converted_pmi, dict):\n",
    "                pmi_df = pd.DataFrame.from_dict(converted_pmi, orient='index', columns=['PMI_Value'])\n",
    "                pmi_df.index.name = 'Character_Pair'\n",
    "                pmi_df.to_csv(f'./data/{base_filename}_pmi_result.csv', encoding='utf-8')\n",
    "                print(f\"âœ“ Saved pmi_result to: ./data/{base_filename}_pmi_result.csv\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save pmi_result: {e}\")\n",
    "    \n",
    "    # 5. Save total_chars and other numeric values\n",
    "    if 'total_chars' in results and results['total_chars'] is not None:\n",
    "        try:\n",
    "            metadata = {\n",
    "                'total_chars': convert_numpy_types(results.get('total_chars')),\n",
    "                'save_time': datetime.now().isoformat(),\n",
    "                'data_types': {\n",
    "                    'corpus': type(results.get('corpus')).__name__,\n",
    "                    'cooccur_prob': type(results.get('cooccur_prob')).__name__,\n",
    "                    'char_count': type(results.get('char_count')).__name__,\n",
    "                    'pmi_result': type(results.get('pmi_result')).__name__\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(f'./data/{base_filename}_metadata.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved metadata to: ./data/{base_filename}_metadata.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save metadata: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Results have been saved to ./data/{base_filename}_* files\")\n",
    "\n",
    "save_results_separate(res, 'xinlangnews_analysis_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735fa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 PMI values:\n",
      " 1. ('æ·«', 'ç§½'): 20.6244\n",
      " 2. ('èœ¡', 'çƒ›'): 20.6244\n",
      " 3. ('å‡›', 'å†½'): 20.6244\n",
      " 4. ('ç‚œ', 'å†ˆ'): 20.6244\n",
      " 5. ('å¿', 'å¿‘'): 20.6244\n",
      " 6. ('å©', 'åŸš'): 20.6244\n",
      " 7. ('ä¿¯', 'ç°'): 20.6244\n",
      " 8. ('ç‰º', 'ç‰²'): 20.6244\n",
      " 9. ('ç»¸', 'ç¼ª'): 20.6244\n",
      "10. ('èšŒ', 'åŸ '): 20.6244\n",
      "11. ('å’”', 'åš“'): 20.6244\n",
      "12. ('å–‡', 'å­'): 20.6244\n",
      "13. ('å³¥', 'åµ˜'): 20.6244\n",
      "14. ('é‚›', 'å´ƒ'): 20.6244\n",
      "15. ('æ¶Ÿ', 'æ¼ª'): 20.6244\n",
      "16. ('é¹Œ', 'é¹‘'): 20.6244\n",
      "17. ('å±', 'å‘€'): 19.6244\n",
      "18. ('è¯½', 'è°¤'): 19.6244\n",
      "19. ('æ¼©', 'æ¶¡'): 19.6244\n",
      "20. ('è§Š', 'è§'): 19.6244\n",
      "21. ('æ¯“', 'æ¬'): 19.6244\n",
      "22. ('ç‘•', 'ç–µ'): 19.6244\n",
      "23. ('æ£', 'è‚˜'): 19.6244\n",
      "24. ('å¿Œ', 'æƒ®'): 19.6244\n",
      "25. ('æ†§', 'æ†¬'): 19.6244\n",
      "26. ('å¡', 'å’¯'): 19.6244\n",
      "27. ('ç’€', 'ç’¨'): 19.6244\n",
      "28. ('æŸ ', 'æª¬'): 19.6244\n",
      "29. ('è™', 'è '): 19.6244\n",
      "30. ('æ¡', 'æ¢'): 19.6244\n",
      "31. ('é¦…', 'é¥¼'): 19.6244\n",
      "32. ('å–§', 'åš£'): 19.6244\n",
      "33. ('æ¡¢', 'éœ„'): 19.6244\n",
      "34. ('å“­', 'æ³£'): 19.6244\n",
      "35. ('æ¸º', 'èŒ«'): 19.6244\n",
      "36. ('ç®', 'å’’'): 19.6244\n",
      "37. ('è¤¶', 'çš±'): 19.6244\n",
      "38. ('èŠ—', 'æ–‹'): 19.6244\n",
      "39. ('èš‚', 'èš'): 19.0394\n",
      "40. ('è•Š', 'ç»®'): 19.0394\n",
      "41. ('è´ª', 'å©ª'): 19.0394\n",
      "42. ('èœ‚', 'çª'): 19.0394\n",
      "43. ('é¸µ', 'é¸Ÿ'): 19.0394\n",
      "44. ('ç’Ÿ', 'ç‘„'): 19.0394\n",
      "45. ('è ', 'ä¾ '): 19.0394\n",
      "46. ('ç‹', 'ç‹¸'): 19.0394\n",
      "47. ('è¶', 'å‘¤'): 19.0394\n",
      "48. ('æ™¦', 'æ¶©'): 19.0394\n",
      "49. ('è‡', 'èŒ'): 19.0394\n",
      "50. ('å€š', 'ä»—'): 19.0394\n",
      "PMI results have been saved to: ./data/xinlangnews_pmi_analysis.json and ./data/xinlangnews_pmi_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pmi_result_special(pmi_result, filename):\n",
    "    \"\"\"\n",
    "    A function specifically for saving PMI results\n",
    "    \"\"\"\n",
    "    # Convert numpy types\n",
    "    converted_pmi = convert_numpy_types(pmi_result)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(f'./data/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_pmi, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save as sorted CSV\n",
    "    if isinstance(converted_pmi, dict):\n",
    "        # Sort by PMI value\n",
    "        sorted_pmi = sorted(converted_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        pmi_df = pd.DataFrame(sorted_pmi, columns=['Character_Pair', 'PMI_Value'])\n",
    "        pmi_df.to_csv(f'./data/{filename}.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        # Also save top N highest PMI values\n",
    "        top_n = min(50, len(sorted_pmi))\n",
    "        top_pmi = sorted_pmi[:top_n]\n",
    "        \n",
    "        print(f\"Top {top_n} PMI values:\")\n",
    "        for i, (pair, value) in enumerate(top_pmi, 1):\n",
    "            print(f\"{i:2d}. {pair}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"PMI results have been saved to: ./data/{filename}.json and ./data/{filename}.csv\")\n",
    "# Specifically save PMI results\n",
    "save_pmi_result_special(res['pmi_result'], 'xinlangnews_pmi_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f607aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI analysis results (top 20):\n",
      "==================================================\n",
      "Rank Character Pair PMI Value \n",
      "--------------------------------------------------\n",
      "1    ('æ·«', 'ç§½') 20.6244\n",
      "2    ('èœ¡', 'çƒ›') 20.6244\n",
      "3    ('å‡›', 'å†½') 20.6244\n",
      "4    ('ç‚œ', 'å†ˆ') 20.6244\n",
      "5    ('å¿', 'å¿‘') 20.6244\n",
      "6    ('å©', 'åŸš') 20.6244\n",
      "7    ('ä¿¯', 'ç°') 20.6244\n",
      "8    ('ç‰º', 'ç‰²') 20.6244\n",
      "9    ('ç»¸', 'ç¼ª') 20.6244\n",
      "10   ('èšŒ', 'åŸ ') 20.6244\n",
      "11   ('å’”', 'åš“') 20.6244\n",
      "12   ('å–‡', 'å­') 20.6244\n",
      "13   ('å³¥', 'åµ˜') 20.6244\n",
      "14   ('é‚›', 'å´ƒ') 20.6244\n",
      "15   ('æ¶Ÿ', 'æ¼ª') 20.6244\n",
      "16   ('é¹Œ', 'é¹‘') 20.6244\n",
      "17   ('å±', 'å‘€') 19.6244\n",
      "18   ('è¯½', 'è°¤') 19.6244\n",
      "19   ('æ¼©', 'æ¶¡') 19.6244\n",
      "20   ('è§Š', 'è§') 19.6244\n",
      "\n",
      "Statistics:\n",
      "Total character pairs: 181393\n",
      "Highest PMI value: 20.6244\n",
      "Lowest PMI value: -8.5090\n",
      "Average PMI value: 1.6269\n",
      "Median PMI value: 1.0865\n"
     ]
    }
   ],
   "source": [
    "def analyze_pmi_results(pmi_result, top_k=20):\n",
    "    \"\"\"\n",
    "    Analyze and display the PMI results\n",
    "    \"\"\"\n",
    "    converted_pmi = convert_numpy_types(pmi_result)\n",
    "    \n",
    "    if isinstance(converted_pmi, dict):\n",
    "        # Sort by PMI value\n",
    "        sorted_pmi = sorted(converted_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"PMI analysis results (top {top_k}):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{'Rank':<4} {'Character Pair':<10} {'PMI Value':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (pair, value) in enumerate(sorted_pmi[:top_k], 1):\n",
    "            print(f\"{i:<4} {str(pair):<10} {value:.4f}\")\n",
    "        \n",
    "        # Statistics\n",
    "        values = [v for _, v in sorted_pmi]\n",
    "        print(\"\\nStatistics:\")\n",
    "        print(f\"Total character pairs: {len(sorted_pmi)}\")\n",
    "        print(f\"Highest PMI value: {max(values):.4f}\")\n",
    "        print(f\"Lowest PMI value: {min(values):.4f}\")\n",
    "        print(f\"Average PMI value: {np.mean(values):.4f}\")\n",
    "        print(f\"Median PMI value: {np.median(values):.4f}\")\n",
    "\n",
    "# Analyze your PMI results\n",
    "analyze_pmi_results(res['pmi_result'], top_k=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC7201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
