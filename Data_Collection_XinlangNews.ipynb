{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80d7ca5",
   "metadata": {},
   "source": [
    "## Data Collection,Data Engineering & Cleaning \n",
    "Our team uses its own dataset obtained through web crawling.\n",
    "\n",
    "In this ipynb file, the main task we have accomplished is to retrieve the latest about 1000 news headlines and their contents from Sina News.And we also perform data cleaning and management.\n",
    "\n",
    "https://news.sina.com.cn/roll/#pageid=153&lid=2509&k=&num=50&page={i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028ad675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5437147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_with_selenium(url,title_href_dict,k):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0\"\n",
    ")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  \n",
    "        print(f\"Crawling page {k}...\")\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for x in soup.find_all('a',{'target':['_blank']},href=True):\n",
    "            for y in x:\n",
    "                title_href_dict[y] = x['href'].strip()\n",
    "        del title_href_dict['意见反馈留言板']\n",
    "        return title_href_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Selenium {url}: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "def crawl_sina_article(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=random.uniform(3, 5))\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        content_div = soup.find('div', id='artibody') or \\\n",
    "                      soup.find('div', class_='article') or \\\n",
    "                      soup.find('div', class_='content')\n",
    "        \n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "        \n",
    "        for tag in content_div(['script', 'style', 'a', 'img', 'iframe']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        text = content_div.get_text(strip=True)\n",
    "        cleaned = re.sub(r'[^\\u4e00-\\u9fa5，。！？；：、‘’“”（）【】\\n\\s]', '', text)\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "        return \"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3a6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1...\n",
      "Crawling page 2...\n",
      "Crawling page 3...\n",
      "Crawling page 4...\n",
      "Crawling page 5...\n",
      "Crawling page 6...\n",
      "Crawling page 7...\n",
      "Crawling page 8...\n",
      "Crawling page 9...\n",
      "Crawling page 10...\n",
      "Crawling page 11...\n",
      "Crawling page 12...\n",
      "Crawling page 13...\n",
      "Crawling page 14...\n",
      "Crawling page 15...\n",
      "Crawling page 16...\n",
      "Crawling page 17...\n",
      "Crawling page 18...\n",
      "Crawling page 19...\n",
      "Crawling page 20...\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    title_href_dict = dict()\n",
    "    for i in range(1,21):\n",
    "        title_url = f\"https://news.sina.com.cn/roll/#pageid=153&lid=2509&k=&num=50&page={i}\"\n",
    "        crawl_with_selenium(title_url,title_href_dict,i)\n",
    "    data_dict = dict()\n",
    "    for title,url in title_href_dict.items():\n",
    "        data_dict[title] = crawl_sina_article(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fef1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 975 entries, 0 to 974\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   文本标题    975 non-null    object\n",
      " 1   文本内容    975 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['文本标题','文本内容'])\n",
    "#remove the part after \"海量资讯、精准解读\"\n",
    "for title,content in data_dict.items():\n",
    "    cleaned_content = re.split(r'海量资讯、精准解读', content)[0].strip()\n",
    "    df.loc[len(df.index)] = [title, cleaned_content]\n",
    "    \n",
    "df.info()\n",
    "df.to_csv('./data/xinlangnews.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC7201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
