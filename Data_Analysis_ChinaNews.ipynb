{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a13dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import jieba  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6d69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean a single text: retain Chinese characters, common punctuation, remove HTML tags, ads, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Retain Chinese characters, common punctuation, numbers, letters (optional)\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€˜â€™â€œâ€ï¼ˆï¼‰ã€ã€‘\\n\\s0-9a-zA-Z]', '', text)\n",
    "    # Remove extra spaces and newlines\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def preprocess_corpus(df):\n",
    "    \"\"\"\n",
    "    Preprocess the entire DataFrame and return a list containing all cleaned texts.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title_clean = clean_text(row['æ–‡æœ¬æ ‡é¢˜'])\n",
    "        content_clean = clean_text(row['æ–‡æœ¬å†…å®¹'])\n",
    "        # Merge the title and content and separate them with spaces\n",
    "        full_text = f\"{title_clean} {content_clean}\"\n",
    "        corpus.append(full_text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22efd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cooccurrence_prob(corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    Calculate the probability of any two Chinese characters co-occurring adjacently P(c_j | c_i)\n",
    "    Returns a dictionary: {(c_i, c_j): probability}\n",
    "    \"\"\"\n",
    "    char_count = defaultdict(int)      # Character frequency\n",
    "    cooccur_count = defaultdict(lambda: defaultdict(int))  # Co-occurrence frequency\n",
    "\n",
    "    for text in corpus:\n",
    "        # Iterate over each character\n",
    "        for i in range(len(text) - window_size + 1):\n",
    "            window = text[i:i+window_size]\n",
    "            if len(window) == window_size:\n",
    "                c1, c2 = window[0], window[1]\n",
    "                char_count[c1] += 1\n",
    "                cooccur_count[c1][c2] += 1\n",
    "\n",
    "    # Calculate the conditional probability P(c_j | c_i)\n",
    "    prob_dict = {}\n",
    "    for c1 in cooccur_count:\n",
    "        for c2 in cooccur_count[c1]:\n",
    "            prob_dict[(c1, c2)] = cooccur_count[c1][c2] / char_count[c1]\n",
    "\n",
    "    return prob_dict, char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pmi(cooccur_prob_dict, char_count, total_chars):\n",
    "    \"\"\"\n",
    "    Calculate pointwise mutual information PMI(c_i, c_j) = log2( P(c_i, c_j) / (P(c_i) * P(c_j)) )\n",
    "    Note: Here P(c_i, c_j) = P(c_j|c_i) * P(c_i)\n",
    "    \"\"\"\n",
    "    pmi_dict = {}\n",
    "    for (c1, c2), cooccur_prob in cooccur_prob_dict.items():\n",
    "        p_c1 = char_count[c1] / total_chars\n",
    "        p_c2 = char_count[c2] / total_chars\n",
    "        # Joint probability P(c1,c2) = P(c2|c1) * P(c1)\n",
    "        p_joint = cooccur_prob * p_c1\n",
    "        if p_c1 > 0 and p_c2 > 0 and p_joint > 0:\n",
    "            pmi = np.log2(p_joint / (p_c1 * p_c2))\n",
    "            pmi_dict[(c1, c2)] = pmi\n",
    "    return pmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fddc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_vector(article_text, vocab):\n",
    "    \"\"\"\n",
    "    Convert the article into a word frequency vector (based on the given vocabulary list)\n",
    "    \"\"\"\n",
    "    # use jieba for tokenization\n",
    "    words = jieba.lcut(article_text)\n",
    "    \n",
    "    word_count = Counter(words)\n",
    "    vector = np.array([word_count.get(word, 0) for word in vocab])\n",
    "    # Normalize to probability distribution\n",
    "    if np.sum(vector) > 0:\n",
    "        vector = vector / np.sum(vector)\n",
    "    return vector\n",
    "\n",
    "\n",
    "def avg_mutual_information_precise(article1, article2, vocab):\n",
    "    \"\"\"\n",
    "    To calculate the average mutual information between two articles more precisely (a joint distribution needs to be constructed)\n",
    "    This method calculates the \"average mutual information at the lexical level\".\n",
    "    \"\"\"\n",
    "    vec1 = article_to_vector(article1, vocab)\n",
    "    vec2 = article_to_vector(article2, vocab)\n",
    "    \n",
    "    # Construct joint distribution (simplified: assume words are independent, but they are not)\n",
    "    # The true joint distribution requires counting the co-occurrence of each pair of words in the two articles.\n",
    "    # Since we only have two articles, we cannot construct a reliable joint distribution.\n",
    "    # Therefore, we switch to calculating the \"average pointwise mutual information of word co-occurrence\".\n",
    "    \n",
    "    # Extract all words from the two articles\n",
    "    words1 = set(jieba.lcut(article1) if 'jieba' in globals() else list(article1))\n",
    "    words2 = set(jieba.lcut(article2) if 'jieba' in globals() else list(article2))\n",
    "    \n",
    "    # Calculate the PMI of the intersecting words (requires global corpus statistics)\n",
    "    # Since we do not have a global corpus, here we can only provide a framework.\n",
    "    # In actual applications, the global char_count and cooccur_count calculated earlier should be used.\n",
    "    \n",
    "    # We output a simple metric based on word overlap\n",
    "    overlap = len(words1 & words2) / len(words1 | words2)  # Jaccard similarity\n",
    "    return overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8776bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main flow function: From the original DataFrame to all calculation results\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning...\")\n",
    "    corpus = preprocess_corpus(df)\n",
    "    print(f\"Cleaning completed, total {len(corpus)} articles\")\n",
    "\n",
    "    print(\"\\nStarting calculation of co-occurrence probabilities...\")\n",
    "    cooccur_prob, char_count = calculate_cooccurrence_prob(corpus)\n",
    "    total_chars = sum(char_count.values())\n",
    "    print(f\"Total {len(char_count)} unique Chinese characters counted\")\n",
    "\n",
    "    print(\"\\nStarting calculation of pointwise mutual information...\")\n",
    "    pmi_result = calculate_pmi(cooccur_prob, char_count, total_chars)\n",
    "    print(f\"Calculated PMI for {len(pmi_result)} pairs of Chinese characters\")\n",
    "\n",
    "    print(\"\\nStarting calculation of average mutual information between articles...\")\n",
    "    # Calculate the average mutual information between the first two articles as an example\n",
    "    if len(corpus) >= 2:\n",
    "        article1 = corpus[0]\n",
    "        article2 = corpus[1]\n",
    "        vocab = list(set(list(article1) + list(article2)))\n",
    "        avg_mi = avg_mutual_information_precise(article1, article2, vocab)\n",
    "        print(f\"Average mutual information between Article 1 and Article 2 (Jaccard approximation): {avg_mi:.6f}\")\n",
    "    else:\n",
    "        print(\"Insufficient number of articles to calculate inter-article mutual information\")\n",
    "\n",
    "    # Return all results for further analysis or saving\n",
    "    return {\n",
    "        'corpus': corpus,\n",
    "        'cooccur_prob': cooccur_prob,\n",
    "        'char_count': char_count,\n",
    "        'pmi_result': pmi_result,\n",
    "        'total_chars': total_chars\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b505f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Cleaning completed, total 965 articles\n",
      "\n",
      "Starting calculation of co-occurrence probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/v2/ptjfpzrd3pz9s7p0353vdcvw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3968 unique Chinese characters counted\n",
      "\n",
      "Starting calculation of pointwise mutual information...\n",
      "Calculated PMI for 165590 pairs of Chinese characters\n",
      "\n",
      "Starting calculation of average mutual information between articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.248 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mutual information between Article 1 and Article 2 (Jaccard approximation): 0.061090\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df = pd.read_csv('./data/chinanews.csv')\n",
    "    res = main_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2a4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved corpus to: ./data/chinanews_analysis_results_corpus.txt\n",
      "âœ“ Saved cooccur_prob to: ./data/chinanews_analysis_results_cooccur_prob.json\n",
      "âœ“ Saved char_count to: ./data/chinanews_analysis_results_char_count.json\n",
      "âœ“ Saved pmi_result to: ./data/chinanews_analysis_results_pmi_result.json\n",
      "âœ“ Saved pmi_result to: ./data/chinanews_analysis_results_pmi_result.csv\n",
      "âœ“ Saved metadata to: ./data/chinanews_analysis_results_metadata.json\n",
      "\n",
      "ğŸ‰ Results have been saved to separate chinanews_analysis_results_* files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy data types to native Python types\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {str(k): convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # For other numpy types\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_results_separate(results, base_filename):\n",
    "    \"\"\"\n",
    "    Save different components in appropriate formats (fixed version)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Save corpus (text data)\n",
    "    if 'corpus' in results and results['corpus'] is not None:\n",
    "        try:\n",
    "            with open(f'./data/{base_filename}_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "                if isinstance(results['corpus'], list):\n",
    "                    for item in results['corpus']:\n",
    "                        f.write(str(item) + '\\n')\n",
    "                else:\n",
    "                    f.write(str(results['corpus']))\n",
    "            print(f\"âœ“ Saved corpus to: ./data/{base_filename}_corpus.txt\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save corpus: {e}\")\n",
    "    \n",
    "    # 2. Save cooccur_prob\n",
    "    if 'cooccur_prob' in results and results['cooccur_prob'] is not None:\n",
    "        try:\n",
    "            if hasattr(results['cooccur_prob'], 'to_csv'):\n",
    "                results['cooccur_prob'].to_csv(f'./data/{base_filename}_cooccur_prob.csv', encoding='utf-8')\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.csv\")\n",
    "            elif isinstance(results['cooccur_prob'], (pd.DataFrame, np.ndarray)):\n",
    "                pd.DataFrame(results['cooccur_prob']).to_csv(f'./data/{base_filename}_cooccur_prob.csv')\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.csv\")\n",
    "            else:\n",
    "                # If it's a dictionary or other type\n",
    "                converted_data = convert_numpy_types(results['cooccur_prob'])\n",
    "                with open(f'./data/{base_filename}_cooccur_prob.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"âœ“ Saved cooccur_prob to: ./data/{base_filename}_cooccur_prob.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save cooccur_prob: {e}\")\n",
    "    \n",
    "    # 3. Save char_count (dictionary data)\n",
    "    if 'char_count' in results and results['char_count'] is not None:\n",
    "        try:\n",
    "            converted_char_count = convert_numpy_types(results['char_count'])\n",
    "            with open(f'./data/{base_filename}_char_count.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(converted_char_count, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved char_count to: ./data/{base_filename}_char_count.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save char_count: {e}\")\n",
    "    \n",
    "    # 4. Save pmi_result (fixed version)\n",
    "    if 'pmi_result' in results and results['pmi_result'] is not None:\n",
    "        try:\n",
    "            # First convert numpy types\n",
    "            converted_pmi = convert_numpy_types(results['pmi_result'])\n",
    "            \n",
    "            # Save as JSON\n",
    "            with open(f'./data/{base_filename}_pmi_result.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(converted_pmi, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved pmi_result to: ./data/{base_filename}_pmi_result.json\")\n",
    "            \n",
    "            # Also save as CSV for easier viewing\n",
    "            if isinstance(converted_pmi, dict):\n",
    "                pmi_df = pd.DataFrame.from_dict(converted_pmi, orient='index', columns=['PMI_Value'])\n",
    "                pmi_df.index.name = 'Character_Pair'\n",
    "                pmi_df.to_csv(f'./data/{base_filename}_pmi_result.csv', encoding='utf-8')\n",
    "                print(f\"âœ“ Saved pmi_result to: ./data/{base_filename}_pmi_result.csv\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save pmi_result: {e}\")\n",
    "    \n",
    "    # 5. Save total_chars and other numeric values\n",
    "    if 'total_chars' in results and results['total_chars'] is not None:\n",
    "        try:\n",
    "            metadata = {\n",
    "                'total_chars': convert_numpy_types(results.get('total_chars')),\n",
    "                'save_time': datetime.now().isoformat(),\n",
    "                'data_types': {\n",
    "                    'corpus': type(results.get('corpus')).__name__,\n",
    "                    'cooccur_prob': type(results.get('cooccur_prob')).__name__,\n",
    "                    'char_count': type(results.get('char_count')).__name__,\n",
    "                    'pmi_result': type(results.get('pmi_result')).__name__\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(f'./data/{base_filename}_metadata.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ“ Saved metadata to: ./data/{base_filename}_metadata.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save metadata: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Results have been saved to separate {base_filename}_* files\")\n",
    "\n",
    "\n",
    "save_results_separate(res, 'chinanews_analysis_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a788aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 PMI values:\n",
      " 1. ('å¦Š', 'å¨ '): 21.1805\n",
      " 2. ('è¸Œ', 'èº‡'): 21.1805\n",
      " 3. ('å‘‹', 'å–¹'): 21.1805\n",
      " 4. ('æµ', 'ç'): 21.1805\n",
      " 5. ('çº', 'çºˆ'): 21.1805\n",
      " 6. ('é”±', 'é“¢'): 21.1805\n",
      " 7. ('è¢ˆ', 'è£Ÿ'): 21.1805\n",
      " 8. ('å“€', 'æ‚¼'): 21.1805\n",
      " 9. ('ç§¸', 'ç§†'): 21.1805\n",
      "10. ('èƒ«', 'è…“'): 21.1805\n",
      "11. ('åŒ', 'åŒ'): 21.1805\n",
      "12. ('é¥¥', 'é¦‘'): 21.1805\n",
      "13. ('æ¶…', 'æ§ƒ'): 21.1805\n",
      "14. ('ç°¸', 'ç®•'): 21.1805\n",
      "15. ('èœ‰', 'è£'): 21.1805\n",
      "16. ('å¾œ', 'å¾‰'): 21.1805\n",
      "17. ('ç–™', 'ç˜©'): 21.1805\n",
      "18. ('ç˜´', 'ç—¼'): 21.1805\n",
      "19. ('å£¬', 'ç…'): 21.1805\n",
      "20. ('äºµ', 'æ¸'): 21.1805\n",
      "21. ('é¸³', 'é¸¯'): 21.1805\n",
      "22. ('è¤¶', 'çš±'): 21.1805\n",
      "23. ('å¿', 'å¿‘'): 21.1805\n",
      "24. ('å’¯', 'å™”'): 21.1805\n",
      "25. ('å½·', 'å¾¨'): 21.1805\n",
      "26. ('ç²±', 'é¥´'): 21.1805\n",
      "27. ('å‡›', 'å†½'): 21.1805\n",
      "28. ('çŒ–', 'ç—'): 21.1805\n",
      "29. ('é¹¦', 'é¹‰'): 21.1805\n",
      "30. ('é†œ', 'æ–'): 21.1805\n",
      "31. ('è€„', 'è€‹'): 21.1805\n",
      "32. ('ç¯ ', 'å¡š'): 21.1805\n",
      "33. ('è¸Ÿ', 'è¹°'): 21.1805\n",
      "34. ('æ', 'æƒš'): 20.1805\n",
      "35. ('é›‡', 'ä½£'): 20.1805\n",
      "36. ('ä»', 'ä»º'): 20.1805\n",
      "37. ('é', 'è¿©'): 20.1805\n",
      "38. ('æ˜³', 'æ˜¤'): 20.1805\n",
      "39. ('æœ¦', 'èƒ§'): 20.1805\n",
      "40. ('æ°¤', 'æ°²'): 20.1805\n",
      "41. ('å¾˜', 'å¾Š'): 20.1805\n",
      "42. ('èƒ³', 'è†Š'): 20.1805\n",
      "43. ('å†¤', 'æ‰'): 20.1805\n",
      "44. ('å ', 'å¨¥'): 20.1805\n",
      "45. ('æ¡', 'æ¢'): 20.1805\n",
      "46. ('è€¸', 'å³™'): 20.1805\n",
      "47. ('éœ“', 'è£³'): 20.1805\n",
      "48. ('è›¤', 'èœŠ'): 20.1805\n",
      "49. ('æ„§', 'ç–š'): 20.1805\n",
      "50. ('é¦„', 'é¥¨'): 20.1805\n",
      "PMI results have been saved to: ./data/chinanews_pmi_analysis.json and ./data/chinanews_pmi_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pmi_result_special(pmi_result, filename):\n",
    "    \"\"\"\n",
    "    A function specifically for saving PMI results\n",
    "    \"\"\"\n",
    "    # Convert numpy types\n",
    "    converted_pmi = convert_numpy_types(pmi_result)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(f'./data/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_pmi, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save as sorted CSV\n",
    "    if isinstance(converted_pmi, dict):\n",
    "        # Sort by PMI value\n",
    "        sorted_pmi = sorted(converted_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        pmi_df = pd.DataFrame(sorted_pmi, columns=['Character_Pair', 'PMI_Value'])\n",
    "        pmi_df.to_csv(f'./data/{filename}.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        # Also save top N highest PMI values\n",
    "        top_n = min(50, len(sorted_pmi))\n",
    "        top_pmi = sorted_pmi[:top_n]\n",
    "        \n",
    "        print(f\"Top {top_n} PMI values:\")\n",
    "        for i, (pair, value) in enumerate(top_pmi, 1):\n",
    "            print(f\"{i:2d}. {pair}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"PMI results have been saved to: ./data/{filename}.json and ./data/{filename}.csv\")\n",
    "# Specifically save PMI results\n",
    "save_pmi_result_special(res['pmi_result'], 'chinanews_pmi_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e05201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI analysis results (top 20):\n",
      "==================================================\n",
      "Rank Character Pair PMI Value \n",
      "--------------------------------------------------\n",
      "1    ('å¦Š', 'å¨ ') 21.1805\n",
      "2    ('è¸Œ', 'èº‡') 21.1805\n",
      "3    ('å‘‹', 'å–¹') 21.1805\n",
      "4    ('æµ', 'ç') 21.1805\n",
      "5    ('çº', 'çºˆ') 21.1805\n",
      "6    ('é”±', 'é“¢') 21.1805\n",
      "7    ('è¢ˆ', 'è£Ÿ') 21.1805\n",
      "8    ('å“€', 'æ‚¼') 21.1805\n",
      "9    ('ç§¸', 'ç§†') 21.1805\n",
      "10   ('èƒ«', 'è…“') 21.1805\n",
      "11   ('åŒ', 'åŒ') 21.1805\n",
      "12   ('é¥¥', 'é¦‘') 21.1805\n",
      "13   ('æ¶…', 'æ§ƒ') 21.1805\n",
      "14   ('ç°¸', 'ç®•') 21.1805\n",
      "15   ('èœ‰', 'è£') 21.1805\n",
      "16   ('å¾œ', 'å¾‰') 21.1805\n",
      "17   ('ç–™', 'ç˜©') 21.1805\n",
      "18   ('ç˜´', 'ç—¼') 21.1805\n",
      "19   ('å£¬', 'ç…') 21.1805\n",
      "20   ('äºµ', 'æ¸') 21.1805\n",
      "\n",
      "Statistics:\n",
      "Total character pairs: 165590\n",
      "Highest PMI value: 21.1805\n",
      "Lowest PMI value: -9.1123\n",
      "Average PMI value: 2.1507\n",
      "Median PMI value: 1.6918\n"
     ]
    }
   ],
   "source": [
    "def analyze_pmi_results(pmi_result, top_k=20):\n",
    "    \"\"\"\n",
    "    Analyze and print the PMI results\n",
    "    \"\"\"\n",
    "    converted_pmi = convert_numpy_types(pmi_result)\n",
    "    \n",
    "    if isinstance(converted_pmi, dict):\n",
    "        # Sort by PMI value\n",
    "        sorted_pmi = sorted(converted_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"PMI analysis results (top {top_k}):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{'Rank':<4} {'Character Pair':<10} {'PMI Value':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (pair, value) in enumerate(sorted_pmi[:top_k], 1):\n",
    "            print(f\"{i:<4} {str(pair):<10} {value:.4f}\")\n",
    "        \n",
    "        # Some basic statistics\n",
    "        values = [v for _, v in sorted_pmi]\n",
    "        print(\"\\nStatistics:\")\n",
    "        print(f\"Total character pairs: {len(sorted_pmi)}\")\n",
    "        print(f\"Highest PMI value: {max(values):.4f}\")\n",
    "        print(f\"Lowest PMI value: {min(values):.4f}\")\n",
    "        print(f\"Average PMI value: {np.mean(values):.4f}\")\n",
    "        print(f\"Median PMI value: {np.median(values):.4f}\")\n",
    "\n",
    "# Analyze PMI results\n",
    "analyze_pmi_results(res['pmi_result'], top_k=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC7201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
