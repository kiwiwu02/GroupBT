{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016138a8",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "Our team uses its own dataset obtained through web crawling.\n",
    "\n",
    "In this ipynb file, the main task we have accomplished is to retrieve the latest about 1000 news headlines and their contents from  ChineseNews.And we also perform data cleaning and management.\n",
    "\n",
    "https://www.chinanews.com.cn/scroll-news/news1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c6a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01676af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1...\n",
      "Crawling page 2...\n",
      "Crawling page 3...\n",
      "Crawling page 4...\n",
      "Crawling page 5...\n",
      "Crawling page 6...\n",
      "Crawling page 7...\n",
      "Crawling page 8...\n",
      "Crawling page 9...\n",
      "Crawling page 10...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_URL = \"https://www.chinanews.com.cn\"\n",
    "def crawl_with_selenium(title_url, title_href_dict):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    try:\n",
    "        driver.get(title_url)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        content_list = soup.find('div', class_='content_list')\n",
    "        if not content_list:\n",
    "            return\n",
    "        for item in content_list.find_all('li'):\n",
    "            classes = item.get('class', [])\n",
    "            if isinstance(classes, str):\n",
    "                classes = [classes]\n",
    "            if 'nocontent' in classes:\n",
    "                continue\n",
    "            title_block = item.find('div', class_='dd_bt')\n",
    "            if not title_block:\n",
    "                continue\n",
    "            a_tag = title_block.find('a')\n",
    "            if not a_tag:\n",
    "                continue\n",
    "            title_text = a_tag.get_text().strip()\n",
    "            href = a_tag.get('href')\n",
    "            if not href:\n",
    "                continue\n",
    "            full_url = urljoin(BASE_URL, href)\n",
    "            title_href_dict[title_text] = full_url\n",
    "    finally:\n",
    "        driver.quit()\n",
    "def crawl_content(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        return \"\"\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    container = soup.find('div', class_='content_maincontent_content')\n",
    "    if container:\n",
    "        main_content = container.find('div', class_='left_zw')\n",
    "        if main_content:\n",
    "            container = main_content\n",
    "    else:\n",
    "        container = soup\n",
    "    text_blocks = []\n",
    "    for block in container.find_all(['p', 'div'], recursive=True):\n",
    "        classes = block.get('class', [])\n",
    "        if isinstance(classes, str):\n",
    "            classes = [classes]\n",
    "        if any(cls in {'adInContent', 'adEditor', 'channel'} for cls in classes):\n",
    "            continue\n",
    "        text = block.get_text(strip=True)\n",
    "        if text:\n",
    "            text_blocks.append(text)\n",
    "    return \"\\n\".join(text_blocks)\n",
    "if __name__=='__main__':\n",
    "    title_href_dict = {}\n",
    "    for i in range(1, 11):\n",
    "        title_url = f\"{BASE_URL}/scroll-news/news{i}.html\"\n",
    "        print(f\"Crawling page {i}...\")\n",
    "        crawl_with_selenium(title_url, title_href_dict)\n",
    "    data_rows = []\n",
    "    for title, url in title_href_dict.items():\n",
    "        content = crawl_content(url)\n",
    "        cleaned_content = content.replace('\\n', '')\n",
    "        # Remove editor notes at the end\n",
    "        cleaned_content = re.sub(r'【编辑:.*?】$', '', cleaned_content).strip()\n",
    "        data_rows.append({'title': title, 'content': cleaned_content})\n",
    "    df = pd.DataFrame(data_rows, columns=['title', 'content'])\n",
    "    df.to_csv('./data/chinanews.csv', index=False, header=['文本标题', '文本内容'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cdb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 965 entries, 0 to 964\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    965 non-null    object\n",
      " 1   content  965 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC7201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
